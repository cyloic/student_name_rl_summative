#  SmartSort RL Agent

> A Reinforcement Learning approach to intelligent waste classification with low-latency decision-making

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Stable-Baselines3](https://img.shields.io/badge/stable--baselines3-latest-green.svg)](https://stable-baselines3.readthedocs.io/)
[![Gymnasium](https://img.shields.io/badge/gymnasium-latest-orange.svg)](https://gymnasium.farama.org/)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Environment Details](#environment-details)
- [Training Results](#training-results)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ¯ Overview

SmartSort RL Agent is a reinforcement learning project that addresses the challenge of **low-latency, high-accuracy waste classification** in resource-constrained environments like Rwanda's waste management system. Instead of building a traditional supervised learning classifier, this project uses RL to learn an **optimal decision-making policy** that balances:

- âš¡ **Latency**: Minimizing feature refinement steps
- ğŸ¯ **Accuracy**: Achieving high-confidence correct classifications
- ğŸ§  **Intelligence**: Learning when to refine features vs. when to classify

The system compares three RL algorithms: **DQN** (Value-Based), **PPO**, and **A2C** (Policy Gradient methods).

---

## âœ¨ Features

- ğŸ”„ **Sequential Decision-Making**: Agent learns when to refine features or make final classification
- ğŸ“Š **Real-time Visualization**: Pygame-based visual feedback of agent's decision process
- ğŸ† **Multiple RL Algorithms**: Comparison of DQN, PPO, and A2C implementations
- ğŸ“ˆ **Comprehensive Logging**: TensorBoard integration for training metrics
- ğŸ® **Interactive Demo**: Watch trained agents classify waste in real-time
- ğŸ’¾ **Model Persistence**: Save and load trained models for evaluation

---

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- Virtual environment (recommended)

### Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/cyloic/student_name_rl_summative.git
   cd cyusa_loic_rl_summative
   ```

2. **Create and activate virtual environment**
   ```bash
   # Windows
   python -m venv .venv
   .venv\Scripts\activate

   # Linux/Mac
   python3 -m venv .venv
   source .venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

### Required Packages

```txt
gymnasium>=0.29.0
stable-baselines3>=2.0.0
pygame>=2.5.0
numpy>=1.24.0
tensorboard>=2.13.0
torch>=2.0.0
```

---

## âš¡ Quick Start

### Run Pre-trained Agent Demo

```bash
python main.py
```

This will:
1. Load the champion DQN model
2. Open a Pygame window showing real-time classification
3. Run 10 test episodes with visual feedback
4. Display classification results and rewards

### Train Your Own Agent

```bash
# Train DQN agent
python training/dqn_training.py --timesteps 50000

# Train PPO agent
python training/pg_training.py --algorithm ppo --timesteps 50000

# Train A2C agent
python training/pg_training.py --algorithm a2c --timesteps 50000
```

### View Training Metrics

```bash
tensorboard --logdir=./logs
```

Then open `http://localhost:6006` in your browser.

---

## ğŸ“ Project Structure

```
cyusa_loic_rl_summative/
â”‚
â”œâ”€â”€ environment/
â”‚   â”œâ”€â”€ custom_env.py            # Custom Gymnasium environment implementation
â”‚   â””â”€â”€ rendering.py             # Visualization GUI components (Pygame)
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ dqn_training.py          # Training script for DQN using Stable-Baselines3
â”‚   â””â”€â”€ pg_training.py           # Training script for PPO/A2C using Stable-Baselines3
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dqn/                     # Saved DQN models
â”‚   â”‚   â””â”€â”€ DQN_Run_10_LR5e-04_G0.99_E0.3.zip
â”‚   â””â”€â”€ pg/                      # Saved policy gradient models
â”‚       â”œâ”€â”€ PPO_Run_1_Best.zip
â”‚       â””â”€â”€ A2C_Run_1_Best.zip
â”‚
â”œâ”€â”€ main.py                      # Entry point for running best performing model
â”œâ”€â”€ requirements.txt             # Project dependencies
â””â”€â”€ README.md                    # Project documentation
```

---

## ğŸ® Environment Details

### Agent
The **SmartSort Classifier Decision Module** interprets intermediate feature vectors and decides between:
- Further refinement (focus on texture/shape)
- Final classification (plastic or paper)

### Action Space (Discrete - 4 actions)
1. `Focus on Texture` - Refinement action
2. `Focus on Shape/Edges` - Refinement action
3. `Classify as Plastic` - Terminal action
4. `Classify as Paper` - Terminal action

### Observation Space (Continuous - 4D vector)
```
[Texture Feature, Shape Feature, Confidence_Plastic, Confidence_Paper]
```

### Reward Structure
| Event | Reward | Purpose |
|-------|--------|---------|
| âœ… Correct Classification | +1000 | Encourage accuracy |
| âŒ Incorrect Classification | -1000 | Penalize errors |
| ğŸ”„ Refinement Action | -10 | Discourage latency |
| ğŸ“ˆ Confidence Increase | +5 | Reward learning progress |

---

## ğŸ“Š Training Results

### Algorithm Comparison

| Algorithm | Mean Reward | Mean Episode Length | Training Stability |
|-----------|-------------|--------------------|--------------------|
| **PPO** | **955.4** â­ | 3.0 steps | High âœ… |
| **A2C** | 948.7 | 3.4 steps | Medium âš ï¸ |
| **DQN** | 924.9 | 3.5 steps | Low âš ï¸ |

### Champion Model Performance

The DQN champion model (`DQN_Run_10_LR5e-04_G0.99_E0.3`) achieved:
- **100% Accuracy** on test episodes
- **3 steps** for Paper classification (optimal)
- **6-8 steps** for Plastic classification
- **Average reward: 983.0** across 20 test episodes

### Key Findings
- ğŸ“„ **Paper items** are classified faster (3 steps) due to clearer features
- ğŸ¥¤ **Plastic items** require more refinement (6-8 steps) due to initial ambiguity
- ğŸ¯ PPO showed best training stability with highest peak performance
- ğŸ”„ DQN demonstrated excellent generalization despite training volatility

---

## Performance Analysis

### Convergence Speed
- PPO converged to 95%+ accuracy in ~20k timesteps
- A2C required ~30k timesteps
- DQN showed high variance, stabilizing after 35k timesteps

### Exploration-Exploitation Analysis
- DQN: Îµ-greedy (Îµ=0.1-0.3) provided consistent exploration
- PPO: Entropy regularization (0.0001) balanced exploration naturally
- A2C: Advantage estimation led to adaptive exploration

### Algorithm Insights
1. PPO's stability advantage due to clipped objective
2. DQN's value function approach vulnerable to overestimation
3. A2C's advantage estimation provided good gradients


## ğŸ’» Usage

### Basic Usage

```python
from environment.custom_env import SmartSortEnv
from stable_baselines3 import DQN

# Create environment
env = SmartSortEnv()

# Load trained model
model = DQN.load("models/dqn/DQN_Run_10_LR5e-04_G0.99_E0.3")

# Run inference
obs, info = env.reset()
for _ in range(100):
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    
    if terminated or truncated:
        print(f"Episode finished with reward: {reward}")
        obs, info = env.reset()
```

### Advanced Training

```python
from environment.custom_env import SmartSortEnv
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback

# Create environment
env = SmartSortEnv()

# Configure model
model = PPO(
    "MlpPolicy",
    env,
    learning_rate=1e-3,
    gamma=0.99,
    n_steps=1024,
    verbose=1,
    tensorboard_log="./logs/PPO"
)

# Setup evaluation callback
eval_callback = EvalCallback(
    env,
    best_model_save_path="./models/pg/",
    log_path="./logs/",
    eval_freq=5000
)

# Train
model.learn(total_timesteps=50000, callback=eval_callback)
```

---

## ğŸ“ Academic Context

**Course**: Reinforcement Learning Summative Assignment  
**Student**: Cyusa Loic  
**Project Focus**: Comparing Value-Based (DQN) vs Policy Gradient (PPO, A2C) methods for sequential decision-making in classification tasks

### Research Questions
1. Which RL algorithm learns the most efficient classification policy?
2. Can RL agents balance latency and accuracy in ambiguous classification scenarios?
3. How do different algorithms handle the exploration-exploitation trade-off?

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---


## ğŸ™ Acknowledgments

- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/) for RL implementations
- [Gymnasium](https://gymnasium.farama.org/) for environment framework
- [Pygame](https://www.pygame.org/) for visualization
- Rwanda's waste management challenges as project inspiration

---

## ğŸ“§ Contact

**Cyusa Loic**  
- GitHub: [cyloic](https://github.com/cyloic)
- Email: l.cyusa@alustudent.com
- Project Link: [https://github.com/cyloic/cyusa_loic_rl_summative](https://github.com/yourusername/cyusa_loic_rl_summative)
- Demo Video : https://www.youtube.com/watch?v=BDUgbt6hHSE
---

## ğŸ¥ Demo

[Link to your 3-minute video demonstration]

---




